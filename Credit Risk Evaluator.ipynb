{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Data\n",
    "\n",
    "The data is located in the Challenge Files Folder:\n",
    "\n",
    "* `lending_data.csv`\n",
    "\n",
    "Import the data using Pandas. Display the resulting dataframe to confirm the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_size</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>borrower_income</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>num_of_accounts</th>\n",
       "      <th>derogatory_marks</th>\n",
       "      <th>total_debt</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.672</td>\n",
       "      <td>52800</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>6.692</td>\n",
       "      <td>43600</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.963</td>\n",
       "      <td>46100</td>\n",
       "      <td>0.349241</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.664</td>\n",
       "      <td>52700</td>\n",
       "      <td>0.430740</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10800.0</td>\n",
       "      <td>7.698</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_size  interest_rate  borrower_income  debt_to_income  num_of_accounts  \\\n",
       "0    10700.0          7.672            52800        0.431818                5   \n",
       "1     8400.0          6.692            43600        0.311927                3   \n",
       "2     9000.0          6.963            46100        0.349241                3   \n",
       "3    10700.0          7.664            52700        0.430740                5   \n",
       "4    10800.0          7.698            53000        0.433962                5   \n",
       "\n",
       "   derogatory_marks  total_debt  loan_status  \n",
       "0                 1       22800            0  \n",
       "1                 0       13600            0  \n",
       "2                 0       16100            0  \n",
       "3                 1       22700            0  \n",
       "4                 1       23000            0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "credit_riskdf = pd.read_csv(\"Resources/lending_data.csv\")\n",
    "credit_riskdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Model Performance\n",
    "\n",
    "You will be creating and comparing two models on this data: a Logistic Regression, and a Random Forests Classifier. Before you create, fit, and score the models, make a prediction as to which model you think will perform better. You do not need to be correct! \n",
    "\n",
    "Write down your prediction in the designated cells in your Jupyter Notebook, and provide justification for your educated guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the text in this markdown cell with your predictions, and be sure to provide justification for your guess.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "X = credit_riskdf.drop(columns=['loan_status'])\n",
    "Y = credit_riskdf['loan_status']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Fit and Compare Models\n",
    "\n",
    "Create a Logistic Regression model, fit it to the data, and print the model's score. Do the same for a Random Forest Classifier. You may choose any starting hyperparameters you like. \n",
    "\n",
    "Which model performed better? How does that compare to your prediction? Write down your results and thoughts in the designated markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        loan_size  interest_rate  borrower_income  debt_to_income  \\\n",
       " 0          9100.0          7.006            46500        0.354839   \n",
       " 1          9800.0          7.300            49300        0.391481   \n",
       " 2         11600.0          8.068            56500        0.469027   \n",
       " 3          8900.0          6.910            45600        0.342105   \n",
       " 4          7800.0          6.443            41200        0.271845   \n",
       " ...           ...            ...              ...             ...   \n",
       " 112535    20000.0         11.642            90200        0.667406   \n",
       " 112536    20200.0         11.698            90700        0.669239   \n",
       " 112537    15800.0          9.844            73200        0.590164   \n",
       " 112538    17900.0         10.732            81600        0.632353   \n",
       " 112539    19100.0         11.246            86400        0.652778   \n",
       " \n",
       "         num_of_accounts  derogatory_marks  total_debt  \n",
       " 0                     3                 0       16500  \n",
       " 1                     4                 0       19300  \n",
       " 2                     5                 1       26500  \n",
       " 3                     3                 0       15600  \n",
       " 4                     2                 0       11200  \n",
       " ...                 ...               ...         ...  \n",
       " 112535               13                 3       60200  \n",
       " 112536               13                 3       60700  \n",
       " 112537                9                 2       43200  \n",
       " 112538               11                 2       51600  \n",
       " 112539               12                 2       56400  \n",
       " \n",
       " [112540 rows x 7 columns],\n",
       " 0         0\n",
       " 1         0\n",
       " 2         0\n",
       " 3         0\n",
       " 4         0\n",
       "          ..\n",
       " 112535    1\n",
       " 112536    1\n",
       " 112537    1\n",
       " 112538    1\n",
       " 112539    1\n",
       " Name: loan_status, Length: 112540, dtype: int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_r, y_r = ros.fit_resample(X_train, Y_train)\n",
    "X_r, y_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9921756775347366\n",
      "Testing Score: 0.9916941807676434\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model and print the model score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=0)\n",
    "model_fit = model.fit(X_train, Y_train)\n",
    "training_score = model_fit.score(X_train, Y_train)\n",
    "testing_score = model_fit.score(X_test, Y_test)\n",
    "\n",
    "### END SOLUTION \n",
    "\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18661   105]\n",
      " [   56   562]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predictions for the test set\n",
    "Y_pred = model_fit.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mat = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True negatives (TN): 18661 , which means the number of instances that were correctly predicted as negative (not bad credit).\n",
    "False positives (FP): 105, which means the number of instances that were incorrectly predicted as positive (bad credit) but are actually negative.\n",
    "False negatives (FN): 56, which means the number of instances that were incorrectly predicted as negative (not bad credit) but are actually positive (bad credit).\n",
    "True positives (TP): 562, which means the number of instances that were correctly predicted as positive (bad credit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TP): 562\n",
      " accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "print(f\"(TP): {tp}\")\n",
    "accuracy = (tp + tn)/ (tp + fp + tn + fn)\n",
    "print(f\" accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9944047745923479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9681180354931903"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn_rate = tn/(tn+fp)\n",
    "tn_rate\n",
    "print(tn_rate)\n",
    "\"What is the trueneg +falsepos/everything\"\n",
    "neg_rate = (tn + fp)/ (tp + fp + tn + fn)\n",
    "neg_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A high negative rate is generally desirable as it means that the classifier is able to identify most of the negatives correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.843\n",
      "Recall: 0.909\n",
      "f1_score: 0.875\n"
     ]
    }
   ],
   "source": [
    "\"\"\"precision and recall can give a better understanding of how well the model is performing in terms of identifying bad credits.\"\"\"\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Get the predictions for the test set\n",
    "Y_pred = model_fit.predict(X_test)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "# spec = (TN/TN *FP)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "f1_score = 2*(precision*recall)/(precision + recall)\n",
    "print(f\"f1_score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9974549456596505\n",
      "Testing Score: 0.991384647131655\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest_fit = forest.fit(X_train, Y_train)\n",
    "ftraining_score = forest_fit.score(X_train, Y_train)\n",
    "ftesting_score = forest_fit.score(X_test, Y_test)\n",
    "\n",
    "### END SOLUTION \n",
    "\n",
    "print(f\"Training Score: {ftraining_score}\")\n",
    "print(f\"Testing Score: {ftesting_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18667    99]\n",
      " [   68   550]]\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions for the test set\n",
    "Y_predf = forest_fit.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "fconfusion_mat = confusion_matrix(Y_test, Y_predf)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(fconfusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TP): 550\n",
      " accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = fconfusion_mat.ravel()\n",
    "print(f\"(TP): {tp}\")\n",
    "accuracy = (tp + tn)/ (tp + fp + tn + fn)\n",
    "print(f\" accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9947245017584995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9681180354931903"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn_rate = tn/(tn+fp)\n",
    "tn_rate\n",
    "print(tn_rate)\n",
    "\"What is the trueneg +falsepos/everything\"\n",
    "neg_rate = (tn + fp)/ (tp + fp + tn + fn)\n",
    "neg_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.847\n",
      "Recall: 0.890\n",
      "f1_score: 0.868\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions for the test set\n",
    "\n",
    "# Calculate precision\n",
    "precisionf = precision_score(Y_test, Y_predf)\n",
    "\n",
    "# Calculate recall\n",
    "recallf = recall_score(Y_test, Y_predf)\n",
    "# spec = (TN/TN *FP)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precisionf:.3f}\")\n",
    "print(f\"Recall: {recallf:.3f}\")\n",
    "f1_scoref = 2*(precisionf*recallf)/(precisionf + recallf)\n",
    "print(f\"f1_score: {f1_scoref:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which model performed better? How does that compare to your prediction? Replace the text in this markdown cell with your answers to these questions.*\n",
    "I predicted that the Linear Regression Model may have been better suited to handle this dataset as there isn't too many features for this classification. According to the values of precision, recall, and f_1 score; the Logistic Regression Model performed a bit better. The random forest classifier has a slightly lower precision and recall but a similar f1-score compared to the logistic regression model. My prediciton was correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
